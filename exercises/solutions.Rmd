Solutions
------------------------------
``` {r setup, cache=FALSE, include=FALSE}
opts_knit$set(echo=FALSE, message=FALSE, progress=FALSE, 
              verbose=FALSE, tidy=TRUE)
opts_knit$set(aliases=c(h='fig.height', w='fig.width',
                cap='fig.cap', scap='fig.scap'))
opts_knit$set(eval.after = c('fig.cap','fig.scap'))
opts_chunk$set(cache=TRUE)
knit_hooks$set(document = function(x) {
  gsub('(\\\\end\\{knitrout\\}[\n]+)', '\\1\\\\noindent ', x)
})
library(knitcitations)
cite_options(tooltip=FALSE)
bib <- read.bibtex("../refs.bib")
options(width=80)
library(latticeExtra)
my.col <- c('cornflowerblue', 'chartreuse3', 'darkgoldenrod1', 'peachpuff3',
            'mediumorchid2', 'turquoise3', 'wheat4', 'slategray2')
trellis.par.set(custom.theme.2())
trellis.par.set(plot.symbol=list(pch=19, cex=1.2),
                strip.background = list(col = "transparent"), 
                fontsize = list(text = 16, points = 8))
set.seed(101)
```

Data sets used in the following exercises can be found on [Github](https://github.com/cogmaster-stats/r-cogstats/tree/master/data).

### R language and descriptive statistics

**Exercise 1.** It is easy to store a series of values in a variable `x`. The missing value, coded as `NA`, is written as is (no surrounding quote).
```{r}
x <- c(5.4, 6.1, 6.2, NA, 6.2, 5.6, 19.0, 6.3)
x
```

To replace the 7th value, we can do
```{r}
x[7] <- NA
x
```

There are now two missing values. The command `mean(x, na.rm=TRUE)` would compute the mean on all observations, and it can be used to impute missing values like this:
```{r}
x[is.na(x)] <- mean(x, na.rm=TRUE)
x
```
It would be possible to write `x[c(4,7)] <- mean(x, na.rm=TRUE)`, but this means we know the index position of the missing values. Using `is.na()` returns a boolean, which can be used to affect every `TRUE` values, for example.

**Exercise 2.** Here are two methods to create a factor with a specific arrangement of levels. First, we can generate the base pattern (three `std` followed by three `new`) and replicate it to obtain the desired length:
```{r}
tx <- factor(rep(rep(c("std","new"), each=3), 10))
```

However, since allocation of levels follows a regular pattern, it is easier to use the `gl()` command. E.g., 
```{r}
tx <- gl(2, 3, 60, labels=c("std","new"))
head(tx, n=7)
```

To work with levels, there are two dedicated commands in R: `levels()` and `relevel()`. Here are some examples of use:
```{r}
levels(tx)[1] <- "old"
tx <- relevel(tx, ref="new")
head(tx <- sample(tx), n=7)
```

It is important to note that we don't have to (and we shouldn't) work with the whole vector, but only update the factor levels. In R, factor are stored as numbers (starting with 1), and labels are just strings associated to each number. Also, variable assignation (`<-`) can be done inside another expression, as shown in the last command.

**Exercise 3.** The `data()` command allows to import any data set that comes with R base and add-on packages. We can either load the package using, e.g., `library(MASS)`, and then `data()`'s the data set, or use directly
```{r}
data(birthwt, package="MASS")
```

It is always a good idea to look for the associated help file, if it exists, with the `help()` command:
```{r, eval=FALSE}
help(birthwt, package="MASS")
```

There are a number of binary variables, taking values in {0,1}. They might be kept as numeric but once converted them to factors with readable labels it is easier to work with them. Note that the `within()` command allows to update variables inside a data frame without prefixing them systematically with the `$` operator. Finally, we will also convert mothers' weight in kilograms.
```{r}
yesno <- c("No","Yes")
ethn <- c("White","Black","Other")
birthwt <- within(birthwt, {
  low <- factor(low, labels=yesno)
  race <- factor(race, labels=ethn)
  smoke <- factor(smoke, labels=yesno)
  ui <- factor(ui, labels=yesno)
  ht <- factor(ht, labels=yesno)
})
birthwt$lwt <- birthwt$lwt/2.2
```

The frequency of history of hypertension (`ht`) can be computed based on the results of the `table()` command (divide counts by total number of cases), but there's a more convenient function that will do this automatically: `prop.table()`.
```{r}
table(birthwt$ht)
prop.table(table(birthwt$ht))
```

The average weight of newborns whose mother was smoking (`smoke=1`, now `smoke="Yes"`) during pregnancy but was free of hypertension (`ht=0`, now `ht="No"`) can be obtained by filtering rows and selecting the column of interest:
```{r}
mean(birthwt[birthwt$smoke=="Yes" & birthwt$ht=="No", "bwt"])
```
Another way to perform the same operation relies on the use of `subset()`, which extracts a specific part of a given data set: (Note that in this case there's no need to quote variable names.)
```{r}
sapply(subset(birthwt, smoke == "Yes" & ht == "No", bwt), mean)
```
The use of `sapply()` is now the recommended way to do the above operation, but westill can use `mean(subset(birthwt, smoke == "Yes" & ht == "No", bwt))`, although it will trigger a warning.

To get the five lowest baby weights for mothers with a weight below the first quartile of maternal weights, we need a two-step approach: First, we subset data fulfilling the condition on maternal weights, then sort the results in ascending order.
```{r}
wk.df <- subset(birthwt, lwt < quantile(lwt, probs=.25), bwt)
sapply(wk.df, sort)[1:5]
```
So, the first command select only rows of the data frame where `lwt` is below the first quartile, as computed by `quantile()`, and then we apply the `sort()` function to the filtered data frame. Again, we use the `sapply()` command, but this last command is equivalent to `sort(wk.df$bwt)` (`subset()` returns a data frame with only one column, named `bwt`, as requested).

To recode the `ptl` variable ("number of previous premature labours"), we can proceed as follows:
```{r}
birthwt$ptl2 <- ifelse(birthwt$ptl > 0, 1, 0)
birthwt$ptl2 <- factor(birthwt$ptl2, labels=c("0", "1+"))
with(birthwt, table(ptl2, ptl))
```
The `ifelse()` statement is used to map every strictly positive value of `ptl` to the value 1 ("if"), 0 otherwise ("else"). This new binary variable is also converted to a fator with meaningful labels. Lastly, we check that everything went ok by cross-classifying both variables.

Assuming the `lattice` package has been loaded using `library(lattice)`, the distribution of individual values can be visualized as follows:
```{r, fig.height=5}
histogram(~ bwt | ptl2, data=birthwt, xlab="Baby weight (g)")
```

Finally, here is a possible solution to display all continuous variables as boxplots:
```{r, eval=FALSE}
is.num <- sapply(birthwt, is.numeric)
wk.df <- birthwt[,which(is.num)]
wk.df <- sapply(wk.df, scale)
library(reshape2)
bwplot(value ~ variable, melt(data.frame(wk.df)))
```

> Try to "read" the above code: what do `is.numeric()` returns, how would you write the second command using `subset()`, why do we `scale()` the data, whta is the purpose of the `reshape()` command?

**Exercise 4.** The simulated data set is reproduced below.
```{r}
d <- data.frame(height = rnorm(40, 170, 10),
                class = sample(LETTERS[1:2], 40, rep=TRUE))
d$height[sample(1:40, 1)] <- 220
```

A first way to tackle this problem is to rely on indexation:
```{r}
d$class[which(d$height == max(d$height))]
```
We first inspect which element corresponds to the maximum value of `height`: `d$height == max(d$height)` returns a vector of booleans of length `length(d$height)`. We then ask for the position of the only TRUE value using `which()`.

It is also possible to sort the entire data frame, and return the last element of the `class` column. See the online help for `order()` for more information about sorting strategies in R.
```{r}
d[do.call(order, d),"class"][40]
```

**Exercise 5.** As with many "unknown" data sets, it is recommended to first check how data were stored (header, record separator, decimal point, etc.), how variables were recorded (numeric or factor), and if there are some unexpected values. In what follows, we won't be concerned with incongruous values: we will just discard them and set them to missing.
```{r}
WD <- "../data"
lung <- read.table(paste(WD, "lungcancer.txt", sep="/"),
                   header=TRUE, na.strings=".")
str(lung)
summary(lung)  
head(sort(lung$time))
head(sort(lung$age))
table(lung$cens)
table(lung$vital.capac)
lung <- within(lung, {
  time[time < 0] <- NA
  age[age == 5] <- NA
  cens[cens == 2] <- NA
  cens <- factor(cens)
  levels(vital.capac)[2:3] <- "low"
})
summary(lung)
```

> Study the above code and try to translate it into your own words.

### Data exploration and two-group comparisons

**Exercise 6.** Like in Exercice 5, it is always a good idea to look at the raw data, or a subset thereof (especially when data files are really big), in a simple text editor. The `reading2.csv` file looks like this:

    Treatment,Response
    Treated,24
    Treated,43
    Treated,58
    Treated,71
    Treated,43
    Treated,49
    Treated,61
    Treated,44
    Treated,.

It can be seen that: there is a header line (names of the variables), each line corresponds to one observation, with current status (Treated or not) and a value for the response variable; missing value seems to be coded as ".". Thus, a command like this should work:
```{r}
reading <- read.csv("../data/reading2.csv", na.strings=".")
head(reading)
summary(reading)
```

The `head()` and `summary()` commands are used to get a basic feeling of how the data look like once imported in R. The latter also provides the number of missing values for each variable. Another way to compute missing observations is to use the `is.na()` function: it returns a boolean value for every observation, and we can simply count the number of positive matches (`TRUE`) using the `sum()`, as illustrated below:
```{r}
sum(is.na(reading$Response))
```
If we want to compute the number of missing data for the two groups, we could use
```{r}
sum(is.na(reading$Response[reading$Treatment == "Control"]))
sum(is.na(reading$Response[reading$Treatment == "Treated"]))
```
but it is easier to rely on aggregating functions, like `tapply()` or `by()`. However, since we used a combination of two commands (`sum()` and `is.na()`), we need to write a little helper function, say `nmiss()`, which will compute the number of missing data for a given vector of values.
```{r}
nmiss <- function(x) sum(is.na(x))
tapply(reading$Response, reading$Treatment, nmiss)
```
It is also possible to use `aggregate()`, but we must remind that this function automatically discard missing values for its computation. So we would need to call it like this:
```{r, eval=FALSE}
aggregate(Response ~ Treatment, reading, nmiss, na.action=na.pass)
```

Sample size is readily obtained using the `table()` command:
```{r}
table(reading$Treatment)
```

Assuming the `lattice` package is already loaded, we can use a density plot as follows:
```{r, fig.height=5}
densityplot(~ Response, data=reading, groups=Treatment, auto.key=TRUE)
```
The `auto.key=TRUE` option ensures that R will draw the corresponding legend.

A Student t-test can be done as shown below:
```{r}
t.test(Response ~ Treatment, data=reading, var.equal=TRUE)
```
The results suggest that there is no evidence of a statistically significant difference (at the 5% level) in average response between the two groups for this particular sample. 

We can further verify group variances and distributions as follows:
```{r, fig.height=5}
aggregate(Response ~ Treatment, data=reading, var)
bwplot(Response ~ Treatment, data=reading, pch="|")
```

The Wilcoxon-Mann-Whitney test, which is a non-parametric procedure relying on ranks of the observations and testing for a location shift between two samples, is available in the `wilcox.test()` command:
```{r, warning = FALSE}
wilcox.test(Response ~ Treatment, data=reading)
```

> How would you explain that the two tests give different outcome with this data set?

**Exercise 7.** The 'fusion' data can be imported in R using the `read.table()` command; records are separated by blanks (tab or space, it doesn't really matter here), and there's no header.
```{r}
fus <- read.table("../data/fusion.dat", header=FALSE)
names(fus) <- c("resp", "grp")
str(fus)
```

Two informative graphical displays are box and whiskers chart or so called strip plot. The latter offer the advantage to show all data points, possibly with some horizontal or vertical jittering to avoid data overlap and/or alpha transparency.
```{r, eval=2, fig.height=4, fig.width=4}
bwplot(resp ~ grp, data=fus)
stripplot(resp ~ grp, data=fus, jitter.data=TRUE, grid="h", alpha=0.5)
```

To summarize the data, we can write a little helper function that combines the `mean()` and `sd()` commands. For a given vector `x`, the following `f()` function will return the mean and standard deviation of all observations, with a default option to handle missing data.
```{r}
f <- function(x, na.rm=TRUE) c(mean=mean(x, na.rm=na.rm), s=sd(x, na.rm=na.rm))
aggregate(resp ~ grp, data=fus, FUN=f)
```

As can be seen, the standard deviation for the first group is very large compare to the first group, which also has the highest mean. This confirms the right-skewness of the distributions depicted in the preceding figure.

Results from Student and Welch t-test are given below.
```{r}
t.test(resp ~ grp, data=fus, var.equal=TRUE)
t.test(resp ~ grp, data=fus)$p.value
```
The classical test, which assumes equality of variance, does not reach the 5% significance level, contrary to Welch t-test with this particular sample.

To get a more symmetric distribution, a log-transformation can be applied to the raw data. This often helps to stabilize the variance as well.
```{r}
fus$resp.log <- log10(fus$resp)
aggregate(resp.log ~ grp, data=fus, FUN=f)
histogram(~ resp + resp.log | grp, data=fus, breaks="Sturges", scales=list(relation="free"))
t.test(resp.log ~ grp, data=fus, var.equal=TRUE)
```

**Remark:** A more precise procedure, the [Box-Cox transformation](http://en.wikipedia.org/wiki/Power_transform), would yield an optimal value close to 0, therefore compatible with a log transformation.

**Exercise 8.** First, we load the data using the supplied command:
```{r}
brain <- read.table("../data/IQ_Brain_Size.txt", header=FALSE, skip=27, nrows=20)
head(brain, 2)
```
The data file indicates that the variables are given in the following order:

    CCMIDSA: Corpus Collasum Surface Area (cm2)
    FIQ: Full-Scale IQ
    HC: Head Circumference (cm)
    ORDER: Birth Order
    PAIR: Pair ID (Genotype)
    SEX: Sex (1=Male 2=Female)
    TOTSA: Total Surface Area (cm2)
    TOTVOL: Total Brain Volume (cm3)
    WEIGHT: Body Weight (kg)  

We can update our data frame with correct labels for variables, and display a brief summary of data types and values.
```{r}
names(brain) <- tolower(c("CCMIDSA", "FIQ", "HC", "ORDER", 
                          "PAIR", "SEX", "TOTSA", "TOTVOL", 
                          "WEIGHT"))
str(brain)
```

The `order`, `pair`, and `sex` variables are categorical, but they are seen as integers by R. They need to be converted as R factors. We will also add more informative label for gender:
```{r}
brain$order <- factor(brain$order)
brain$pair <- factor(brain$pair)
brain$sex <- factor(brain$sex, levels=1:2, labels=c("Male", "Female"))
summary(brain)
```

To compute counts or frequencies of boys and girls, we can use `table()` and/or `prop.table()`. The latter is usually to be preferred.
```{r, eval=c(1,3)}
table(brain$sex)
table(brain$sex) / sum(table(brain$sex))
prop.table(table(brain$sex))
```

Average and median IQ level are available with `summary(brain)`, but they are easily computed as
```{r}
mean(brain$fiq)
median(brain$fiq)
```
The number of children with an IQ < 90 can be found using `table()`:
```{r}
table(brain$fiq < 90)
```
Alternatively, we could use `sum(brain$fiq < 90)`.

Anatomical quantities were also partly summarized with `summary(brain)`. We can find the values of the first and third quartile of CCMIDSA like this:
```{r}
quantile(brain$ccmidsa, probs=c(0.25, 0.75))
diff(quantile(brain$ccmidsa, probs=c(0.25, 0.75)))
IQR(brain$ccmidsa)
```
The inter-quartile range is simply the difference between the third and first quartile, and R already offers a dedicated command: `IQR()`. This command could be applied to each variable. Of note, here is a way to serialize such an operation:
```{r}
sapply(brain[,c("ccmidsa","hc","totsa","totvol")], IQR)
```

The distribution of children weight according to twins number can be summarized with an histogram as shown below.
```{r, fig.height=5}
histogram(~ weight | order, data=brain, type="count", 
          xlab="Weight (kg)", ylab="Counts")
```

Terciles are readily obtained using the same `quantile()` command:
```{r}
quantile(brain$weight, probs=0:3/3)
```
and they can be used to create a three-class variable thanks to the `cut()` command.
```{r}
weight.terc <- cut(brain$weight, breaks=quantile(brain$weight, probs=0:3/3),
                   include.lowest=TRUE)
table(weight.terc)
aggregate(totvol ~ weight.terc, data=brain, FUN=function(x) c(mean(x), sd(x)))
```
The `breaks=` options is used to indicate how to break the continuous variable into class intervals. Importantly, one must specify `include.lowest=TRUE` in order to include the minimum value of the variable, because default intervals are opened on the left. Finally, although we could reuse our preceding `f()` function (Exercice 7), notice that we can provide inline function to the `aggregate()` command.

Since we are working with monozygotic twins who share all their genes, and given that the characteristics under study are neuropsychological traits thought to be highly heritable, we can safely assume a paired t-test whose results are given below:
```{r}
aggregate(fiq ~ order, data=brain, function(x) c(mean(x), sd(x)))
t.test(fiq ~ order, data=brain, paired=TRUE)
```
This confirms that the observed data do not allow us to reject the null hypothesis (no difference between twins regarding IQ level).

Comparing head circumference of males vs. females could rely on a t-test for independent samples if we are willing to assume that boys and girls have different anthropometrical characteristics at birth.
```{r}
aggregate(hc ~ sex, data=brain, function(x) c(mean(x), sd(x)))
t.test(hc ~ sex, data=brain)
```

Finally, here is an alternative to strip plots, which relies on [Cleveland's dotplot](http://goo.gl/iSrlOh):
```{r, fig.height=5}
dotplot(hc ~ sex, data=brain, groups=order, type=c("p", "a"), jitter.x=TRUE, 
        auto.key=list(title="Birth order", cex=.8, cex.title=1, columns=2))
```
Points are colored according to `order` levels, and the `type=c("p", "a")` option asks R to show individual values and average values for each subgroups (i.e., cross-classifying `sex` and `order` levels). This confirms that there seems to be little variation between twins, but that girls generally have a smaller head circumference.

> How would you compute standardized mean difference (Cohen's d) in each case?

### One-way and two-way ANOVA

**Exercise 9.** The working data set is reproduced below.

```{r}
set.seed(101)
k <- 3                   # number of groups 
ni <- 10                 # number of observations per group
mi <- c(10, 12, 8)       # group means
si <- c(1.2, 1.1, 1.1)   # group standard deviations
grp <- gl(k, ni, k * ni, labels = c("A", "B", "C"))
resp <- c(rnorm(ni, mi[1], si[1]), rnorm(ni, mi[2], si[2]), rnorm(ni, mi[3], si[3]))
```

A very basic descriptive summary of the data is provided by `summary()` and group-wise operations:
```{r}
d <- data.frame(grp, resp)
rm(grp, resp)
summary(d)
aggregate(resp ~ grp, data=d, mean)
```
We created a data frame and deleted intermediate variable to keep a clean workspace. The `aggregate()` command returns a data frame; we can store it in a new variable for later use, e.g.
```{r}
resp.means <- aggregate(resp ~ grp, data=d, mean)
```

In particular, we can now use this variable to compute differences between group means (available in column headed `resp` in the `resp.means` data frame) and the grand mean:
```{r}
resp.means$resp - mean(d$resp)
```
These results suggest that the largest difference is between group B and C.

A boxplot is easily drawn using `bwplot()` and the same formula that we used to summarize the data by group:
```{r, fig.height=5}
bwplot(resp ~ grp, data=d)
```

To compute the ANOVA table, we will again use the `resp ~ grp` formula:
```{r}
m <- aov(resp ~ grp, data=d)
summary(m)
```
This significant result suggest that the observed data are not compatible with the null hypothesis. The F-statistic can be computed as follows:
```{r}
fval <- 33.031/0.828  ## MS grp / MS residual, see print(summary(m), digits=5)
pval <- pf(fval, 2, 27, lower.tail=FALSE)
format(pval, digits=5)
```

If we want to discard the `C` level for factor `grp`, we can either subset the whole data frame (e.g., `subset(d, grp != "C")`), or call the `aov()` command with a `subset=` option as shown below:
```{r}
m2 <- aov(resp ~ grp, data=d, subset = grp != "C")
summary(m2)
```

Since we are working with only two groups, this is strictly equivalent to carrying out a Student t-test (assuming equal variance), and the F-statistic is just the square of the t-statistic (p-values will be strictly identical). Here are two ways to write the test for the first two groups:
```{r, eval=1}
t.test(d$resp[d$grp == "A"], d$resp[d$grp == "B"], var.equal=TRUE)
t.test(resp ~ grp, data=subset(d, grp != "C"), var.equal=TRUE)
```
(Another option for subsetting would be to use `subset(d, subset=grp %in% c("A","B"))`.)

**Exercise 10.** The `taste.dat` file is a simple tab-delimited text file, with names of the variables on first line. It can be imported using `read.table()`.
```{r}
taste <- read.table("../data/taste.dat", header=TRUE)
summary(taste)
```

As can be seen, R did not recognize columns 2 and 3 as factors, hence this quick post-processing:
```{r}
taste$SCR <- factor(taste$SCR, levels=0:1, labels=c("coarse", "fine"))
taste$LIQ <- factor(taste$LIQ, levels=0:1, labels=c("low", "high"))
names(taste) <- tolower(names(taste))
summary(taste)
```

A detailed overview for the arrangement of experimental units can be obtained with `replications()`:
```{r}
fm <- score ~ scr * liq
replications(fm, data=taste)
```
Note that since we will be reusing the same formula for the ANOVA model, we can associate it to a dedicated variable. Using `*` instead of `+` is not a problem with `aggregate()`. Again, let's define a little helper function that returns sample size, mean, and standard deviation.
```{r}
f <- function(x) c(n=length(x), mean=mean(x), sd=sd(x))
res <- aggregate(fm, data=taste, f)
res
```

Since `aggregate()` always returns a data frame with a column for the numerical quantity of interest (note that there should be only one value, unlike the results produced above) , a dot chart can be drawn by embedding the command directly in the `data=` argument as follows:
```{r, eval=FALSE}
dotplot(score ~ scr, data=aggregate(fm, taste, mean), groups=liq, type="l")
```
However, most `lattice` functions are able to compute average values thanks to the `type="a"` option, so the above instruction is equivalent to:
```{r, fig.height=5}
dotplot(score ~ scr, data=taste, groups=liq, type=c("a"), auto.key=TRUE)
```

Box and wiskers charts of scores for the 4 treatments are obtained easily using a simpler formula:
```{r, fig.height=5}
bwplot(score ~ scr +  liq, taste, ablie=list(h=mean(taste$score), lty=2))
```
The grand mean has been added to the above plot to allow comparison of groups relative to a baseline.

The saturated model is shown below. It appears that the interaction is not significant at the 5% level, suggesting that this term could be removed from the model yielding a simpler model with two main effects (`scr + liq`). Likewise, the effect of `liq` is not significant, but it is better to keep it in the ANOVA table since it was included in the original experimental design.
```{r}
m0 <- aov(fm, data=taste)  ## full model
summary(m0)
```

Although we could simply write down `aov(score ~ scr + fiq, taste)`, here is a way to update the previous model which happens to be very handy in regression models including many terms:
```{r}
m1 <- update(m0, . ~ . - scr:liq)  ## reduced model
summary(m1)
```

Obviously, sum of squares remain identical to those computed in the saturated model, only the resiudal term get 1 additional degree of freedom for the two F-tests.

A Bartlett test for the homogeneity of variances could be used to check the assumption of equal variance (although it is barely useful to test a null hypothesis that we cannot accept, and without any *a priori* idea of the parameter location under the alternative). Since the original model include two factors interacting each other, the test should be carried out at the level of the treatment (3 DF), and not for each group separately:
```{r}
bartlett.test(score ~ interaction(liq, scr), data=taste)
```
(An alternative test, the Levene test, is available in the `car` package. Its syntax is closer to the one used to fit the ANOVA model (`leveneTest(fm, data=taste)`).)

The above results suggest that the data do not suggest strong departure from the null hypothesis of equal variances.

Finally, to summarize the effect of `scr`, we can compute the partial $\eta^2$ as follows: (Here we are using the residual SS from the second model.)
```{r}
10609/(10609+5089)  ## 68% of explained variance
```

Note that we would reach a similar conclusion (large effect of the `scr` factor) using a standardized mean difference or a two-group comparison:
```{r}
t.test(score ~ scr, data=taste, var.equal=TRUE)
library(MBESS)
with(taste, smd(score[scr=="coarse"], score[scr=="fine"]))
```

### Correlation and linear regression

**Exercise 11.** A quick look at the raw data file will confirm that missing data are coded as ".". This need to be set a the default NA coding when importing the data.
```{r}
brains <- read.table("../data/brain_size.dat", header=TRUE, 
                     na.strings=".")  
str(brains)
```

Before computing any measure of linear association, it is useful to display a scatterplot of the data with `xyplot()` and a lowess smoother: 
```{r, fig.height=5}
xyplot(MRI_Count ~ Weight, data=brains, pch=as.numeric(brains$Gender), type=c("p", "g", "smooth"), auto.key=TRUE) 
```
We have highlighted gender levels by using separate character symbols: Recall that factor levels are stored as numeric values (starting at 1) with associated labels (sorted in lexicographic order) by R, hence in this case the `pch=` argument will take values 1 (Female, dots) and 2 (Male, triangles):
```{r}
head(as.character(brains$Gender))
head(as.numeric(brains$Gender))
```

Pearson's correlation can be computed using `cor()` without further option. Instead of writing `cor(brains$MRI_Count, brains$Weight)`, we can use the `with()` shortcut which allows to work with data located in a specific data frame.
```{r}
with(brains, cor(MRI_Count, Weight))
```
As can be seen, R will return `NA` since there are missing values in the data. Instead, we need to tell R to compute correlation on complete pairwise observations (which can be abbreviated as `"pair"`, see the online help):
```{r}
with(brains, cor(MRI_Count, Weight, use="pair"))
```

However, the `cor()` command does not provide 95% confidence intervals, so we need to use  `cor.test()`, as shown below. This function offers a formula interface, which might be further exploited to compute results by gender.
```{r}
cor.test(~ MRI_Count + Weight, data=brains)
```

Since we can pass a data frame directly to `cor.test()`,  we can also subset its values by any logical filter. In particular, we could compute Pearson r for males and females as follows:
```{r, eval=1}
cor.test(~ MRI_Count + Weight, data=subset(brains, Gender == "Female"))
cor.test(~ MRI_Count + Weight, data=subset(brains, Gender == "Male"))
```

To test the equality of the two correlation coefficients, we can use the `r.test()` command from the `psych` package. If it has not been installed before, we can simply use `install.packages("psych")`, and then load the package.
```{r}
library(psych)
r.test(20, .4463, -.07687)
```
Nothing in the data allows us to reject the null hypothesis of equality of correlations in the two subgroups (P=0.1).

> Is it correct to use a sample size of 20 while we know that some observations are missing for the two variables under study?

In the preceding figure, we used different symbols to highlight men and women. The `groups=` argument is a better option, especially in the case where we are interested in adding a lowess smoother for each group. In the next figure, we will, however, display a group-specific regression line.
```{r, fig.height=5}
xyplot(MRI_Count ~ Weight, data=brains, groups=Gender, type=c("p", "g", "r"), auto.key=TRUE) 
```
It is clear that a positive linear association only holds for females, although it remains of moderate magnitude. Using the logarithm (base 10) of MRI counts would not change the above picture and conclusions.

A regression model can be fitted as follows:
```{r}
m <- lm(MRI_Count ~ Weight, data=brains)
summary(m)
```
The `confint()` command can be used on almost all models fitted with R, and it returns $100(1-\alpha)$ confidence interval, where confidence level is set as `level=` (default is to use 95%). As can be seen, the 95% CI for the slope parameter does not include 0, in line with results from the t-test, and this suggests that there is a significant linear relationship between the two variables.
```{r}
confint(m)
```

Finally, to verify the assumptions about the distribution of residuals and the constant variance, we could use an histogram and a scatterplot with residuals by fitted values.
```{r, eval=1, fig.height=5}
histogram(~ resid(m))
xyplot(resid(m) ~ fitted(m), abline=list(h=0, lty=2), 
       type=c("p","g","smooth"))
```
Using `densityplot(~ resid(m))` would produce a density plot rather than an histogram.

**Exercise 12.** We will import the data using the `load()` command since the data set is already in R format (`rda` or `RData` extension). Before processing the data to perform feature selection, we will also take a look at the raw data.
```{r}
load("../data/sim.rda")
dim(sim)
sim[1:5,1:5]
```
Note that `sim` is not a data frame, but a matrix (meaning that variables can only be addressed by their number in the table, and not using the `$` operator followed by their name; e.g., `sim[,1]` would return values for `y`, we could also use `sim[,"y"]`, but not `sim$y`).

A scatterplot matrix can also be used to display pairwise relationship between variables, although in this case the high number of variables does not lend itself to such visual display.
```{r, fig.height=5}
splom(~ sim[,1:5], type=c("p","g","smooth"), cex=.6, alpha=.5)
```

We need to compute all pairwise correlations between $y$ and the $x_i$s. This can be done in several ways, but here is how it can be done using a simple `for` loop: we first create a vector holding correlation values, and then iterate over variables 2 to 43 in the data matrix.
```{r}
pv <- numeric(43)
for (i in 2:44) 
  pv[i-1] <- cor.test(sim[,"y"], sim[,i])$p.value
summary(pv)
sort(pv)[1:4]
```
The last command prints the four lowest p-values. If we want to adjust all p-values with Bonferroni method, we can use the following:
```{r}
pvc <- p.adjust(pv, method="bonf")
-log10(pvc[1:4])
sum(pvc <= .05) # -log10(pvc) >= 1.30 
```
Note that it is often useful to work with log-transformed p-values (for plotting purpose).

Univariate screening of candidate predictors are interesting on their own, but as we have seen it is necessary to correct for multiple testing in case the significance of the test (and not, say, an effect size measure) is used to single out irrelevant predictors. Bonferroni-corrected tests are known to be conservative, and it is assumed that tests are independent. Finally, this correlation approach is blind to partial correlation (one $x_i$ associated to $y$ through its correlation with $x_j$).

Lastly, if we consider variables correlating to $y$ at a 5% (uncorrect) level, we can fit a regression model as suggested below. The idea is to create a new data frame holding the `y` response variable and predictors whose column position (minus 1) were stored in the `pv` variable.
```{r}
selx <- which(pv <= .05)
d <- data.frame(sim[,c(1,selx+1)])
summary(lm(y ~ ., data=d))
```

### ANOVA and regression

**Exercise 13.** Data can be loaded and pre-processed very easily:
```{r}
data(ToothGrowth)
ToothGrowth$dose <- factor(ToothGrowth$dose)
head(ToothGrowth$dose)
```

Results from the different models are summarized below: 

- (m1) One-way ANOVA with `dose` treated as a factor with unordered levels.
- (m2) One-way ANOVA with `dose` treated as a factor with ordered levels.
- (m3) Linear regression with `dose` treated as a numeric variable.
- (m4) Linear regression including linear and quadratic effect for `dose`.

```{r}
m1 <- aov(len ~ dose, data=ToothGrowth)
ToothGrowth$dose <- ordered(ToothGrowth$dose)
m2 <- aov(len ~ dose, data=ToothGrowth)
m3 <- lm(len ~ as.numeric(dose), data=ToothGrowth)
m4 <- lm(len ~ as.numeric(dose) + I(as.numeric(dose)^2), data=ToothGrowth)
summary(m1)
summary(m2, split=list(dose=c(1,2)))
anova(m3)
anova(m4)
```

In the first model, the total variance is separated into two sources: dose effect and residuals, while in the second model polynomial contrasts are automatically added and tested by R since `dose` is now an ordered factor. More precicely, the following contrast matrix is used:
```{r}
round(contr.poly(levels(ToothGrowth$dose)), 3)
```
However, the total SS associated to `dose` (2426) remains the same as in `m1`: we are not adding anything to the model, but rather test the 3-level dose effect using two orthogonal contrasts–one for the linear trend (`C1`), and the other for a quadratic trend (`C2`). The results suggest that only the linear component really matters.

The next two models treat dose as a numerical variable with equally spaced values, and not observed values (which would be obtained with `as.numeric(as.character(dose)`). The SSs associated to the linear (`m3` and `m4`) and quadratic (`m4`) are essentially the same as in the ANOVA models. A regression model applied to categorical variables where levels are treated as numerical values can be used to test the linearity of the relationship between the response and explanatory variables.

**Exercise 14.** As data were already loaded in Exercise 10, we will continue from there. We just need to recode `scr` as a numerical predictor.
```{r}
class(taste$scr)
taste$scr <- as.numeric(taste$scr)-1
class(taste$scr)
table(taste$scr)
```
Since factor levels started at 1, we can simply substract 1 after having called `as.numeric` to the `scr` variable. A simple table of counts is helpful to check that we didn't miss any observation. 

A simple scatterplot can be used to visually inspect the joint distribution of the two variables. Adding `type="r"` when calling `xyplot()` will also display a regression line.
```{r, fig.height=5}
xyplot(score ~ scr, data=taste, type=c("p","g","r"), alpha=.75)
```

A regression model can be fitted using `lm()`.
```{r}
m <- lm(score ~ scr, data=taste)
coef(m)
summary(m)
```

The slope of the regression line simply reflects the difference in means between the two conditions since `scr` is now a 0/1 variable. The intercept is equal to the mean in the first condition. Note that the slope wouldn't change if we were to code `scr` as 1/2 instead of 0/1, but the intercept would be different since it is the value of the response variable when the predictor equals 0.
```{r}
aggregate(score ~ scr, data=taste, mean)
diff(aggregate(score ~ scr, data=taste, mean)$score)
```

The `taste$SCORE - fitted(m)` values reflect deviations between observed and fitted values. This is what we call the residuals of the model:
```{r}
head(taste$score - fitted(m))
head(resid(m))
```
Moreover, as can be seen below,
```{r}
unique(fitted(m))
```
predicted values are simply means in each condition.

Compared to a classical t-test,
```{r}
t.test(score ~ scr, data=taste, var.equal=TRUE)
```
except for the sign of the test statistic (which depends on the order of the levels), we find again comparable reference (mean in the reference group and difference of means), and the F-statistic for the regression model is just the square of the above t value.

Now, if we change coding of factor levels using {-1,1} instead of {0,1} the intercept now equals the grand mean (i.e., the average of both group means), and the slope now reflects deviation of mean scores in the second group from the grand mean.
```{r}
taste$scr[taste$scr == 0] <- -1
summary(lm(score ~ scr, data=taste))
mean(taste$score)
with(taste, tapply(score, scr, mean) - mean(score))
```

**Exercise 15.** Let us first load the data as indicated.
```{r}
load("../data/rats.rda")
rat <- within(rat, {
  Diet.Amount <- factor(Diet.Amount, levels=1:2, 
                        labels=c("High","Low"))
  Diet.Type <- factor(Diet.Type, levels=1:3, 
                        labels=c("Beef","Pork","Cereal")) 
  })
```

An interaction plot is readily constructed by plotting the response variable against one of the two predictors, and using the other one as a grouping variable.
```{r, fig.height=5}
xyplot(Weight.Gain ~ Diet.Type, data=rat, groups=Diet.Amount, type=c("a","g"), abline=list(h=mean(rat$Weight.Gain), lty=2), auto.key=TRUE)
```
The horizontal dashed line is the grand mean.

The 6 treatments, `Beef/High`, `Beef/Low`, `Pork/High`, `Pork/Low`, `Cereal/High`, and `Cereal/Low`, correspond to all combination of factor levels. They can be generated as follows:
```{r}
tx <- with(rat, interaction(Diet.Amount, Diet.Type, sep="/"))
head(tx)
unique(table(tx))  # obs / treatment
```

It can be seen that the one-way ANOVA F-test is significant, suggesting that some pairs of means differ when we consider all 6 treatments.
```{r}
m <- aov(Weight.Gain ~ tx, data=rat)
summary(m)
```

To identify pairs of means significantly different at a 5% FWER-corrected level, we can use `pairwise.t.test()`:
```{r}
pairwise.t.test(rat$Weight.Gain, tx, p.adjust="bonf")
```
Results from these tests suggest that difference exist only for `Pork/High` vs. `Pork/Low`, `Beef/High` vs. `Beef/Low`, and `Beef/Low` vs. `Pork/High`, as suggested by the interaction plot.

To build the requested matrix of contrasts, we can use the following commands:
```{r}
ctr <- cbind(c(-1,-1,-1,-1,2,2)/6, 
             c(-1,-1,1,1,0,0)/4,
             c(-1,1,-1,1,-1,1)/6,
             c(1,-1,1,-1,-2,2)/6,  # C1 x C3
             c(1,-1,-1,1,0,0)/4)   # C2 x C3
crossprod(ctr)
```
The last command allows to check that all contrasts are orthogonal one to the other. The last two contrasts allows to test for the interaction between the two factors, and thus are derived from the product of contrasts coding for the main effects of these factors. These constrasts can be directly associated to the `tx` factor, and tested using the `aov()` command. Again we will 'split' the ANOVA table to highlight the different contrasts in the output.
```{r}
contrasts(tx) <- ctr
m <- aov(rat$Weight.Gain ~ tx)
summary(m, split=list(tx=1:5))
```

Let us compare these results with what would be obtained from a two-way ANOVA:
```{r}
summary(aov(Weight.Gain ~ Diet.Type * Diet.Amount, data=rat))
```
The SS associated to the two factors and their interaction equals 4613, and the preceding table suggest that there is no effect of diet type, but that diet amount is slightly interacting with diet type. The F-test for `Diet.Amount` is exactly the same as that of the third contrast stored in `ctr`.

Finally, a contrast opposing `Beef/High` and `Pork/High` to all other treatments can be buld and tested as shown below:
```{r, message=FALSE}
tx <- with(rat, interaction(Diet.Amount, Diet.Type, sep="/"))
C6 <- c(2,-1,2,-1,-1,-1)/6
library(multcomp)
summary(glht(aov(rat$Weight.Gain ~ tx), linfct=mcp(tx=C6)))
```
We gain found a significant effect suggesting that average weight gain in those two conditions is larger than in the other ones.

