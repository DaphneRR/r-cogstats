Solutions
------------------------------
``` {r setup, cache=FALSE, include=FALSE}
opts_knit$set(echo=FALSE, message=FALSE, progress=FALSE, 
              cache=TRUE, verbose=FALSE, tidy=TRUE)
opts_knit$set(aliases=c(h='fig.height', w='fig.width',
                cap='fig.cap', scap='fig.scap'))
opts_knit$set(eval.after = c('fig.cap','fig.scap'))
knit_hooks$set(document = function(x) {
  gsub('(\\\\end\\{knitrout\\}[\n]+)', '\\1\\\\noindent ', x)
})
library(knitcitations)
cite_options(tooltip=FALSE)
bib <- read.bibtex("../refs.bib")
options(width=80)
library(latticeExtra)
my.col <- c('cornflowerblue', 'chartreuse3', 'darkgoldenrod1', 'peachpuff3',
            'mediumorchid2', 'turquoise3', 'wheat4', 'slategray2')
trellis.par.set(custom.theme.2())
trellis.par.set(plot.symbol=list(pch=19, cex=1.2),
                strip.background = list(col = "transparent"), 
                fontsize = list(text = 16, points = 8))
set.seed(101)
```

Data sets used in the following exercises can be found on [Github](https://github.com/cogmaster-stats/r-cogstats/tree/master/data).

### R language and descriptive statistics

**Exercise 1.** It is easy to store a series of values in a variable `x`. The missing value, coded as `NA`, is written as is (no surrounding quote).
```{r}
x <- c(5.4, 6.1, 6.2, NA, 6.2, 5.6, 19.0, 6.3)
x
```

To replace the 7th value, we can do
```{r}
x[7] <- NA
x
```

There are now two missing values. The command `mean(x, na.rm=TRUE)` would compute the mean on all observations, and it can be used to impute missing values like this:
```{r}
x[is.na(x)] <- mean(x, na.rm=TRUE)
x
```
It would be possible to write `x[c(4,7)] <- mean(x, na.rm=TRUE)`, but this means we know the index position of the missing values. Using `is.na()` returns a boolean, which can be used to affect every `TRUE` values, for example.

**Exercise 2.** Here are two methods to create a factor with a specific arrangement of levels. First, we can generate the base pattern (three `std` followed by three `new`) and replicate it to obtain the desired length:
```{r}
tx <- factor(rep(rep(c("std","new"), each=3), 10))
```

However, since allocation of levels follows a regular pattern, it is easier to use the `gl()` command. E.g., 
```{r}
tx <- gl(2, 3, 60, labels=c("std","new"))
head(tx, n=7)
```

To work with levels, there are two dedicated commands in R: `levels()` and `relevel()`. Here are some examples of use:
```{r}
levels(tx)[1] <- "old"
tx <- relevel(tx, ref="new")
head(tx <- sample(tx), n=7)
```

It is important to note that we don't have to (and we shouldn't) work with the whole vector, but only update the factor levels. In R, factor are stored as numbers (starting with 1), and labels are just strings associated to each number. Also, variable assignation (`<-`) can be done inside another expression, as shown in the last command.

**Exercise 3.** The `data()` command allows to import any data set that comes with R base and add-on packages. We can either load the package using, e.g., `library(MASS)`, and then `data()`'s the data set, or use directly
```{r}
data(birthwt, package="MASS")
```

It is always a good idea to look for the associated help file, if it exists, with the `help()` command:
```{r, eval=FALSE}
help(birthwt, package="MASS")
```

There are a number of binary variables, taking values in {0,1}. They might be kept as numeric but once converted them to factors with readable labels it is easier to work with them. Note that the `within()` command allows to update variables inside a data frame without prefixing them systematically with the `$` operator. Finally, we will also convert mothers' weight in kilograms.
```{r}
yesno <- c("No","Yes")
ethn <- c("White","Black","Other")
birthwt <- within(birthwt, {
  low <- factor(low, labels=yesno)
  race <- factor(race, labels=ethn)
  smoke <- factor(smoke, labels=yesno)
  ui <- factor(ui, labels=yesno)
  ht <- factor(ht, labels=yesno)
})
birthwt$lwt <- birthwt$lwt/2.2
```

The frequency of history of hypertension (`ht`) can be computed based on the results of the `table()` command (divide counts by total number of cases), but there's a more convenient function that will do this automatically: `prop.table()`.
```{r}
table(birthwt$ht)
prop.table(table(birthwt$ht))
```

The average weight of newborns whose mother was smoking (`smoke=1`, now `smoke="Yes"`) during pregnancy but was free of hypertension (`ht=0`, now `ht="No"`) can be obtained by filtering rows and selecting the column of interest:
```{r}
mean(birthwt[birthwt$smoke=="Yes" & birthwt$ht=="No", "bwt"])
```
Another way to perform the same operation relies on the use of `subset()`, which extracts a specific part of a given data set: (Note that in this case there's no need to quote variable names.)
```{r}
sapply(subset(birthwt, smoke == "Yes" & ht == "No", bwt), mean)
```
The use of `sapply()` is now the recommended way to do the above operation, but westill can use `mean(subset(birthwt, smoke == "Yes" & ht == "No", bwt))`, although it will trigger a warning.

To get the five lowest baby weights for mothers with a weight below the first quartile of maternal weights, we need a two-step approach: First, we subset data fulfilling the condition on maternal weights, then sort the results in ascending order.
```{r}
wk.df <- subset(birthwt, lwt < quantile(lwt, probs=.25), bwt)
sapply(wk.df, sort)[1:5]
```
So, the first command select only rows of the data frame where `lwt` is below the first quartile, as computed by `quantile()`, and then we apply the `sort()` function to the filtered data frame. Again, we use the `sapply()` command, but this last command is equivalent to `sort(wk.df$bwt)` (`subset()` returns a data frame with only one column, named `bwt`, as requested).

To recode the `ptl` variable ("number of previous premature labours"), we can proceed as follows:
```{r}
birthwt$ptl2 <- ifelse(birthwt$ptl > 0, 1, 0)
birthwt$ptl2 <- factor(birthwt$ptl2, labels=c("0", "1+"))
with(birthwt, table(ptl2, ptl))
```
The `ifelse()` statement is used to map every strictly positive value of `ptl` to the value 1 ("if"), 0 otherwise ("else"). This new binary variable is also converted to a fator with meaningful labels. Lastly, we check that everything went ok by cross-classifying both variables.

Assuming the `lattice` package has been loaded using `library(lattice)`, the distribution of individual values can be visualized as follows:
```{r, fig.height=5}
histogram(~ bwt | ptl2, data=birthwt, xlab="Baby weight (g)")
```

Finally, here is a possible solution to display all continuous variables as boxplots:
```{r, eval=FALSE}
is.num <- sapply(birthwt, is.numeric)
wk.df <- birthwt[,which(is.num)]
wk.df <- sapply(wk.df, scale)
library(reshape2)
bwplot(value ~ variable, melt(data.frame(wk.df)))
```

> Try to "read" the above code: what do `is.numeric()` returns, how would you write the second command using `subset()`, why do we `scale()` the data, whta is the purpose of the `reshape()` command?

**Exercise 4.** The simulated data set is reproduced below.
```{r}
d <- data.frame(height = rnorm(40, 170, 10),
                class = sample(LETTERS[1:2], 40, rep=TRUE))
d$height[sample(1:40, 1)] <- 220
```

A first way to tackle this problem is to rely on indexation:
```{r}
d$class[which(d$height == max(d$height))]
```
We first inspect which element corresponds to the maximum value of `height`: `d$height == max(d$height)` returns a vector of booleans of length `length(d$height)`. We then ask for the position of the only TRUE value using `which()`.

It is also possible to sort the entire data frame, and return the last element of the `class` column. See the online help for `order()` for more information about sorting strategies in R.
```{r}
d[do.call(order, d),"class"][40]
```

**Exercise 5.** As with many "unknown" data sets, it is recommended to first check how data were stored (header, record separator, decimal point, etc.), how variables were recorded (numeric or factor), and if there are some unexpected values. In what follows, we won't be concerned with incongruous values: we will just discard them and set them to missing.
```{r}
WD <- "../data"
lung <- read.table(paste(WD, "lungcancer.txt", sep="/"),
                   header=TRUE, na.strings=".")
str(lung)
summary(lung)  
head(sort(lung$time))
head(sort(lung$age))
table(lung$cens)
table(lung$vital.capac)
lung <- within(lung, {
  time[time < 0] <- NA
  age[age == 5] <- NA
  cens[cens == 2] <- NA
  cens <- factor(cens)
  levels(vital.capac)[2:3] <- "low"
})
summary(lung)
```

> Study the above code and try to translate it into your own words.

### Data exploration and two-group comparisons

**Exercise 6.** Like in Exercice 5, it is always a good idea to look at the raw data, or a subset thereof (especially when data files are really big), in a simple text editor. The `reading2.csv` file looks like this:

    Treatment,Response
    Treated,24
    Treated,43
    Treated,58
    Treated,71
    Treated,43
    Treated,49
    Treated,61
    Treated,44
    Treated,.

It can be seen that: there is a header line (names of the variables), each line corresponds to one observation, with current status (Treated or not) and a value for the response variable; missing value seems to be coded as ".". Thus, a command like this should work:
```{r}
reading <- read.csv("../data/reading2.csv", na.strings=".")
head(reading)
summary(reading)
```

The `head()` and `summary()` commands are used to get a basic feeling of how the data look like once imported in R. The latter also provides the number of missing values for each variable. Another way to compute missing observations is to use the `is.na()` function: it returns a boolean value for every observation, and we can simply count the number of positive matches (`TRUE`) using the `sum()`, as illustrated below:
```{r}
sum(is.na(reading$Response))
```
If we want to compute the number of missing data for the two groups, we could use
```{r}
sum(is.na(reading$Response[reading$Treatment == "Control"]))
sum(is.na(reading$Response[reading$Treatment == "Treated"]))
```
but it is easier to rely on aggregating functions, like `tapply()` or `by()`. However, since we used a combination of two commands (`sum()` and `is.na()`), we need to write a little helper function, say `nmiss()`, which will compute the number of missing data for a given vector of values.
```{r}
nmiss <- function(x) sum(is.na(x))
tapply(reading$Response, reading$Treatment, nmiss)
```
It is also possible to use `aggregate()`, but we must remind that this function automatically discard missing values for its computation. So we would need to call it like this:
```{r, eval=FALSE}
aggregate(Response ~ Treatment, reading, nmiss, na.action=na.pass)
```

Sample size is readily obtained using the `table()` command:
```{r}
table(reading$Treatment)
```

Assuming the `lattice` package is already loaded, we can use a density plot as follows:
```{r, fig.height=5}
densityplot(~ Response, data=reading, groups=Treatment, auto.key=TRUE)
```
The `auto.key=TRUE` option ensures that R will draw the corresponding legend.

A Student t-test can be done as shown below:
```{r}
t.test(Response ~ Treatment, data=reading, var.equal=TRUE)
```
The results suggest that there is no evidence of a statistically significant difference (at the 5% level) in average response between the two groups for this particular sample. 

We can further verify group variances and distributions as follows:
```{r, fig.height=5}
aggregate(Response ~ Treatment, data=reading, var)
bwplot(Response ~ Treatment, data=reading, pch="|")
```

The Wilcoxon-Mann-Whitney test, which is a non-parametric procedure relying on ranks of the observations and testing for a location shift between two samples, is available in the `wilcox.test()` command:
```{r, warning = FALSE}
wilcox.test(Response ~ Treatment, data=reading)
```

> How would you explain that the two tests give different outcome with this data set?

**Exercise 7.** The 'fusion' data can be imported in R using the `read.table()` command; records are separated by blanks (tab or space, it doesn't really matter here), and there's no header.
```{r}
fus <- read.table("../data/fusion.dat", header=FALSE)
names(fus) <- c("resp", "grp")
str(fus)
```

Two informative graphical displays are box and whiskers chart or so called strip plot. The latter offer the advantage to show all data points, possibly with some horizontal or vertical jittering to avoid data overlap and/or alpha transparency.
```{r, eval=2, fig.height=4, fig.width=4}
bwplot(resp ~ grp, data=fus)
stripplot(resp ~ grp, data=fus, jitter.data=TRUE, grid="h", alpha=0.5)
```

To summarize the data, we can write a little helper function that combines the `mean()` and `sd()` commands. For a given vector `x`, the following `f()` function will return the mean and standard deviation of all observations, with a default option to handle missing data.
```{r}
f <- function(x, na.rm=TRUE) c(mean=mean(x, na.rm=na.rm), s=sd(x, na.rm=na.rm))
aggregate(resp ~ grp, data=fus, FUN=f)
```

As can be seen, the standard deviation for the first group is very large compare to the first group, which also has the highest mean. This confirms the right-skewness of the distributions depicted in the preceding figure.

Results from Student and Welch t-test are given below.
```{r}
t.test(resp ~ grp, data=fus, var.equal=TRUE)
t.test(resp ~ grp, data=fus)$p.value
```
The classical test, which assumes equality of variance, does not reach the 5% significance level, contrary to Welch t-test with this particular sample.

To get a more symmetric distribution, a log-transformation can be applied to the raw data. This often helps to stabilize the variance as well.
```{r}
fus$resp.log <- log10(fus$resp)
aggregate(resp.log ~ grp, data=fus, FUN=f)
histogram(~ resp + resp.log | grp, data=fus, breaks="Sturges", scales=list(relation="free"))
t.test(resp.log ~ grp, data=fus, var.equal=TRUE)
```

**Remark:** A more precise procedure, the [Box-Cox transformation](http://en.wikipedia.org/wiki/Power_transform), would yield an optimal value close to 0, therefore compatible with a log transformation.

**Exercise 8.** First, we load the data using the supplied command:
```{r}
brain <- read.table("../data/IQ_Brain_Size.txt", header=FALSE, skip=27, nrows=20)
head(brain, 2)
```
The data file indicates that the variables are given in the following order:

    CCMIDSA: Corpus Collasum Surface Area (cm2)
    FIQ: Full-Scale IQ
    HC: Head Circumference (cm)
    ORDER: Birth Order
    PAIR: Pair ID (Genotype)
    SEX: Sex (1=Male 2=Female)
    TOTSA: Total Surface Area (cm2)
    TOTVOL: Total Brain Volume (cm3)
    WEIGHT: Body Weight (kg)  

We can update our data frame with correct labels for variables, and display a brief summary of data types and values.
```{r}
names(brain) <- tolower(c("CCMIDSA", "FIQ", "HC", "ORDER", 
                          "PAIR", "SEX", "TOTSA", "TOTVOL", 
                          "WEIGHT"))
str(brain)
```

The `order`, `pair`, and `sex` variables are categorical, but they are seen as integers by R. They need to be converted as R factors. We will also add more informative label for gender:
```{r}
brain$order <- factor(brain$order)
brain$pair <- factor(brain$pair)
brain$sex <- factor(brain$sex, levels=1:2, labels=c("Male", "Female"))
summary(brain)
```

To compute counts or frequencies of boys and girls, we can use `table()` and/or `prop.table()`. The latter is usually to be preferred.
```{r, eval=c(1,3)}
table(brain$sex)
table(brain$sex) / sum(table(brain$sex))
prop.table(table(brain$sex))
```

Average and median IQ level are available with `summary(brain)`, but they are easily computed as
```{r}
mean(brain$fiq)
median(brain$fiq)
```
The number of children with an IQ < 90 can be found using `table()`:
```{r}
table(brain$fiq < 90)
```
Alternatively, we could use `sum(brain$fiq < 90)`.

Anatomical quantities were also partly summarized with `summary(brain)`. We can find the values of the first and third quartile of CCMIDSA like this:
```{r}
quantile(brain$ccmidsa, probs=c(0.25, 0.75))
diff(quantile(brain$ccmidsa, probs=c(0.25, 0.75)))
IQR(brain$ccmidsa)
```
The inter-quartile range is simply the difference between the third and first quartile, and R already offers a dedicated command: `IQR()`. This command could be applied to each variable. Of note, here is a way to serialize such an operation:
```{r}
sapply(brain[,c("ccmidsa","hc","totsa","totvol")], IQR)
```

The distribution of children weight according to twins number can be summarized with an histogram as shown below.
```{r, fig.height=5}
histogram(~ weight | order, data=brain, type="count", 
          xlab="Weight (kg)", ylab="Counts")
```

Terciles are readily obtained using the same `quantile()` command:
```{r}
quantile(brain$weight, probs=0:3/3)
```
and they can be used to create a three-class variable thanks to the `cut()` command.
```{r}
weight.terc <- cut(brain$weight, breaks=quantile(brain$weight, probs=0:3/3),
                   include.lowest=TRUE)
table(weight.terc)
aggregate(totvol ~ weight.terc, data=brain, FUN=function(x) c(mean(x), sd(x)))
```
The `breaks=` options is used to indicate how to break the continuous variable into class intervals. Importantly, one must specify `include.lowest=TRUE` in order to include the minimum value of the variable, because default intervals are opened on the left. Finally, although we could reuse our preceding `f()` function (Exercice 7), notice that we can provide inline function to the `aggregate()` command.

Since we are working with monozygotic twins who share all their genes, and given that the characteristics under study are neuropsychological traits thought to be highly heritable, we can safely assume a paired t-test whose results are given below:
```{r}
aggregate(fiq ~ order, data=brain, function(x) c(mean(x), sd(x)))
t.test(fiq ~ order, data=brain, paired=TRUE)
```
This confirms that the observed data do not allow us to reject the null hypothesis (no difference between twins regarding IQ level).

Comparing head circumference of males vs. females could rely on a t-test for independent samples if we are willing to assume that boys and girls have different anthropometrical characteristics at birth.
```{r}
aggregate(hc ~ sex, data=brain, function(x) c(mean(x), sd(x)))
t.test(hc ~ sex, data=brain)
```

Finally, here is an alternative to strip plots, which relies on [Cleveland's dotplot](http://goo.gl/iSrlOh):
```{r, fig.height=5}
dotplot(hc ~ sex, data=brain, groups=order, type=c("p", "a"), jitter.x=TRUE, 
        auto.key=list(title="Birth order", cex=.8, cex.title=1, columns=2))
```
Points are colored according to `order` levels, and the `type=c("p", "a")` option asks R to show individual values and average values for each subgroups (i.e., cross-classifying `sex` and `order` levels). This confirms that there seems to be little variation between twins, but that girls generally have a smaller head circumference.

> How would you compute standardized mean difference (Cohen's d) in each case?

### One-way and two-way ANOVA

**Exercise 9.** The working data set is reproduced below.

```{r}
set.seed(101)
k <- 3                   # number of groups 
ni <- 10                 # number of observations per group
mi <- c(10, 12, 8)       # group means
si <- c(1.2, 1.1, 1.1)   # group standard deviations
grp <- gl(k, ni, k * ni, labels = c("A", "B", "C"))
resp <- c(rnorm(ni, mi[1], si[1]), rnorm(ni, mi[2], si[2]), rnorm(ni, mi[3], si[3]))
```

A very basic descriptive summary of the data is provided by `summary()` and group-wise operations:
```{r}
d <- data.frame(grp, resp)
summary(d)
aggregate(resp ~ grp, data=d, mean)
```

