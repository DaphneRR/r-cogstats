Lab 4 : Regression analysis with R
------------------------------
``` {r setup, cache=FALSE, include=FALSE}
opts_knit$set(echo=FALSE, message=FALSE, progress=FALSE, 
              cache=TRUE, verbose=FALSE, tidy=TRUE)
opts_knit$set(aliases=c(h='fig.height', w='fig.width',
                cap='fig.cap', scap='fig.scap'))
opts_knit$set(eval.after = c('fig.cap','fig.scap'))
knit_hooks$set(document = function(x) {
  gsub('(\\\\end\\{knitrout\\}[\n]+)', '\\1\\\\noindent ', x)
})
library(knitcitations)
cite_options(tooltip=FALSE)
bib <- read.bibtex("../refs.bib")
options(width=80)
library(latticeExtra)
my.col <- c('cornflowerblue', 'chartreuse3', 'darkgoldenrod1', 'peachpuff3',
            'mediumorchid2', 'turquoise3', 'wheat4', 'slategray2')
trellis.par.set(custom.theme.2())
trellis.par.set(plot.symbol=list(pch=19, cex=1.2),
                strip.background = list(col = "transparent"), 
                fontsize = list(text = 16, points = 8))
set.seed(101)
```

This document is written using [R Markdown](http://www.rstudio.com/ide/docs/r_markdown). The source code is available in `04-regression-lab.R`. 


#### Learning objectives

> * Use scatter display and smoother
> * Compute linear and rank correlation
> * Build a linear regression model and interpret model parameters
> * Use basic diagnostic tools for regression modeling

## Scatterplot and correlation

We will consider the data discussed in [Exercise 8](http://rpubs.com/chl/cogmaster-stats-exercises) (see the [solution](http://rpubs.com/chl/cogmaster-stats-solutions)) about the imaging study on brain volume and anthropometric characteristics of Monozygotic twins `r citep(bib["tramo98"])`. The data set was loaded and recoded as follows:
```{r}
brain <- read.table("../data/IQ_Brain_Size.txt", header=FALSE, skip=27, nrows=20)
names(brain) <- tolower(c("CCMIDSA", "FIQ", "HC", "ORDER", 
                          "PAIR", "SEX", "TOTSA", "TOTVOL", 
                          "WEIGHT"))
brain$order <- factor(brain$order)
brain$pair <- factor(brain$pair)
brain$sex <- factor(brain$sex, levels=1:2, labels=c("Male", "Female"))
summary(brain)
```

We will focus on the relationship between two numerical variables: `totvol` (total brain volume, in cm$^3$) and `totsa` (total surface area, in cm$^2$). The analysis will be restricted on the 10 subjects defined by birth order (`order=1`). To simplify command arguments, we can extract the relevant data into a derived data frame, but we could also work directly with `subset()` each time.
```{r}
brain2 <- subset(brain, order == 1, c(totsa, totvol))
```


Before computing any measure of linear association, like Pearson's r, it is highly recommended to plot all data points into a two-dimensional display, also called scatterplot. The basic `lattice` command is the `xyplot()` function, with a formula describing what to plot on y-axis (left-hand side of the formula) and what will go on the x-axis (right-hand side). Many options are available, including `type="p"` (points), `type="l"` (lines), `type="g"` (grid), `type="smooth"` (lowess smoother). Here is an example of use:
```{r, fig.height=5, fig.width=5}
xyplot(totvol ~ totsa, data=brain2, type=c("p", "g", "smooth"), span=1,
       xlab="Total surface area (cm2)", ylab="Total brain volume (cm3)")
```
[Lowess smoother](http://en.wikipedia.org/wiki/Local_regression) belongs to local regression methods that allows to assess the linearity of the relationship between two variables `r citep(bib["cleveland79"])`. The `span=` parameter allows to control the width of the smoothing window. The larger it is, the less the local smoother will adjust to local irregularities. It is often useful to specify a combination of `type="smooth"` and `type="r"` to get lowess and OLS (regression), i.e. `type=c("p", "r", "smooth")`, superimposed on the same panel since it will help to spot possible deviations from the linearity assumption.

Pearson's correlation can be obtained with the `cor()` command.
```{r}
cor(brain2$totvol, brain2$totsa)
```

In case there are missing values, it is necessary to indicate to R how to compute the correlation coefficient. Usually, we add `use="pairwise.complete.obs"` when there are more than 2 variables and we are interested in computing a correlation matrix. Spearman $\rho$ (rank-based measure of correlation, which allows to summarize a monotonic relationship) can be obtained by adding `method="spearman` to the preceding instruction.
```{r}
cor(brain2$totvol, brain2$totsa, method="spearman")
```

It is also possible to assess the statistical significance of Pearson or Spearman's correlation with `cor.test()`, as shown below.
```{r}
cor.test(~ totvol + totsa, data=brain2)
```


## Regression

To fit a simple regression model, we wil use exactly the same formula as that used to display a scatterplot, namely `totvol ~ totsa`. Here are the results produced by R:
```{r}
m <- lm(totvol ~ totsa, data=brain2)
summary(m)
```

R displays the value of the regression coefficient and their associated t-tests. The slope corresponds to the parameter of interest (how does `totvol` vary when `totsa` is increased by one unit), while the intercept is the value of `totvol` when `totsa=0`.

Note that an equivalent call using the full data frame would be: 
```{r, eval=FALSE}
lm(totvol ~ totsa, brain, subset = order == 1)
```

It is also possible to display an ANOVA table for the regression, although most of the information was already given by R at the bottom of the preceding result.
```{r}
anova(m)
```

One of the assumption of this statistical model is that residuals are random normal, meaning that they do not exhibit any particular pattern depedning on the values of the observed or fitted values. A quick look at a residuals by fitted values often helps to confirm the validity of this hypothesis:
```{r, fig.height=4}
xyplot(resid(m) ~ fitted(m), type=c("p", "g"), abline=list(h=0, lty=2),
       xlab="Predicted values (totvol)", ylab="Residuals")
```

Note that the `lattice` package has a built-in command for displaying residual plots, namely `rfs()`.

Finally, several measure of observation influence are available in R. We are usually mainly interested in (univariate) outlying values, observation exhibiting a large leverage effect (x-axis), obsrvation having a large residual in absolute values (y-axis), and observation influencing parameter estimates. See 
[SAS Influence Diagnostics](http://www.sfu.ca/sasdoc/sashtml/stat/chap55/sect38.htm)
or `r citet(bib["fox10"])` for a detailed overview of influence measures. In the next figure, several indices were used, based on recommend cut-off values (see SAS guidelines). Most information used to build these graphics is given by the `fitted()`, `resid()`, and `influence.measures()` commands. Based on the results, the 8th observation seems to be the most influential observation, since it would largely alter the slope of the regression line if removed (see its large DFBETA or Cook's distance value).



```{r, echo=FALSE, message=FALSE}
m.inf <- as.data.frame(influence.measures(m)$infmat)
n <- 10
obs <- 1:n
indiv <- rownames(m.inf)
m.inf$obs <- obs
dfbs.cut <- c(-1,1)*2/sqrt(n)
dffit.cut <- c(-1,1)*2*sqrt(2/n)
cook.cut <- 4/(n-2-1)
p <- list()
p[[1]] <- xyplot(resid(m) ~ fitted(m), abline=list(h=0, lty=2, col="gray30"),
                 type="p", col.line="#BF3030", span=1/3, 
                 xlab="Fitted values", ylab="Residuals",
                 par.settings=custom.theme.2(pch=19))
p[[2]] <- xyplot(rstudent(m) ~ fitted(m), type="p",
                 jitter.x=TRUE, amount=.2, col.line="#BF3030", 
                 ylim=c(-5, 5),
                 xlab="Fitted values", ylab="Studentized residuals",
                 par.settings=custom.theme.2(pch=19),
                 panel=function(x, y, ...) {
                   panel.xyplot(x, y, ...)
                   panel.abline(h=c(-2,2), lty=2, col="gray30")
                   panel.text(x[abs(y)>2],
                              y[abs(y)>2],
                              indiv[abs(y)>2],
                              cex=.8, adj=c(-.5, .5))
                 })
p[[3]] <- xyplot(cook.d ~ obs, data=m.inf, 
                 ylab="Cook's distance", xlab="Observation",
                 ylim=c(-0.1, 1.2),
                 par.settings=custom.theme.2(pch=19),
                 panel=function(x, y, ...) {
                   panel.xyplot(x, y, ...)
                   panel.abline(h=cook.cut, lty=2, col="gray30")
                   panel.text(x[y>cook.cut],
                              y[y>cook.cut],
                              indiv[y>cook.cut],
                              cex=.8, adj=c(-.5, .5))
                 })
p[[4]] <- xyplot(dfb.tots ~ obs, data=m.inf, 
                 ylab="DFBETAS", xlab="Observation",
                 ylim=c(-2.35, 2.35),
                 par.settings=custom.theme.2(pch=19),
                 panel=function(x, y, ...) {
                   panel.xyplot(x, y, ...)
                   panel.abline(h=dfbs.cut, lty=2, col="gray30")
                   panel.text(x[abs(y)>dfbs.cut[2]],
                              y[abs(y)>dfbs.cut[2]],
                              indiv[abs(y)>dfbs.cut[2]],
                              cex=.8, adj=c(-.5, .5))
                 })
library(gridExtra)                 
do.call(grid.arrange, p)
```

## Application

Government statisticians in England conducted a [study of the relationship between smoking and lung cancer](http://lib.stat.cmu.edu/DASL/Stories/SmokingandCancer.html). The data concern 25 occupational groups and are condensed from data on thousands of individual men. The explanatory variable is the number of cigarettes smoked per day by men in each occupation relative to the number smoked by all men of the same age. This smoking ratio is 100 if men in an occupation are exactly average in their smoking, it is below 100 if they smoke less than average, and above 100 if they smoke more than average. The response variable is the standardized mortality ratio for deaths from lung cancer. It is also measured relative to the entire population of men of the same ages as those studied, and is greater or less than 100 when there are more or fewer deaths from lung cancer than would be expected based on the experience of all English men.  

Here are the raw data:
```{r}
smoking <- c(77,137,117,94,116,102,111,93,88,102,91,104,107,112,113,110,125,133,115,105,87,91,100,76,66)
mortality <- c(84,116,123,128,155,101,118,113,104,88,104,129,86,96,144,139,113,146,128,115,79,85,120,60,51)
```

1. Create a data frame, `d`, with the above variables. Provide a numerical summary of the data (range, mean/median, standard deviation, inter-quartile range).
2. Use a scatterplot to display the relationship between the two variables.
3. Add a lowess smoother to this scatterplot, and try different values for the `span=` parameter (e.g., 1/3, 1/2, 2/3, 3/4).
4. Compute Pearson's correlation coefficient, and give a 95% confidence interval for the parameter estimate.
5. Fit a simple regression model, with `mortality` as the response variable. Find Pearson's r from the value of the slope of the regression line.
6. Compute the squared correlation of fitted and observed values. What does this value stand for?

## References

```{r, echo=FALSE, results='asis'}
bibliography(style="text")
```

