---
title: "Categorical data and logistic regression"
author: "Christophe Lalanne"
date: "December 2, 2014"
output:
  ioslides_presentation:
    css: style.css
---
  
```{r, include=FALSE}
library(knitr)
library(ascii)
library(knitcitations)
#cite_options(tooltip=TRUE)
bib <- read.bibtex("../refs.bib")
library(Cairo)
CairoFonts(
  regular = "Fontin Sans:style=Regular",
  bold = "Fontin Sans:style=Bold",
  italic = "Fontin Sans:style=Italic",
  bolditalic = "Fontin Sans:style=Bold Italic,BoldItalic"
)
opts_chunk$set(cache=TRUE, dev="CairoPNG", dev.args = list(bg = 'transparent'))
options(reindent.spaces=2)
library(latticeExtra)
## https://github.com/jennybc/STAT545A
my.col <- c('cornflowerblue', 'chartreuse3', 'darkgoldenrod1', 'peachpuff3',
            'mediumorchid2', 'turquoise3', 'wheat4', 'slategray2')
trellis.par.set(strip.background = list(col = "transparent"), 
                plot.symbol = list(pch = 19, cex = 1.2, col = my.col),
                plot.line = list(lwd = 2, col = my.col[1]),
                superpose.symbol = list(pch = 19, cex = 1.2, col = my.col),
                superpose.line = list(lwd = 2, col = my.col),
                box.rectangle = list(col = "gray60"),
                box.umbrella = list(col = "gray60"),
#                box.dot = list(col = my.col),
                fontsize = list(text = 16, points = 8))
set.seed(101)
```



# Synopsis

---

> Statistics is the grammar of science. Karl Pearson (1857-1936)

<br /><br /><br />
  
<center>
**contingency and square tables • chi-square test • odds-ratio • logistic regression**
</center>

# Analysis of categorical data

## Association between two categorical variables

To describe the assocation between two categorical variables, with I and J levels, we can build a two-way table of counts (or frequencies). Note that we generally need to know the **marginal distribution** of each variable (row and columns totals).

A basic question that we may ask is whether the two variables are related in some way, which amounts to test the null hypothesis that **the two variables are independent**. 

There are several tests available, depending on the kind of data and the type of hypothesis we are interested in `r citep(bib["bishop07"])`.

## Two-way cross-classification table

In the general case, a count or frequency table can be built as shown below. The $n_{i\cdot}$ ($i=1,\dots,I$) and $n_{\cdot j}$ ($j=1,\dots,J$) represent the marginal distributions of variables A and B. Note that if $I=J$, we call it a square table.

![](./img/tab_contingency_60pct.png)

## The chi-square test

Pearson's chi-square test is used to compare **observed data** to **expected data**, that is data occuring under the null hypothesis of no association between two variables. Expected counts are given by the product of margins (if A and B are independant, P(A and B) = P(A) x P(B)). 

Up to a constant, the chi-square statistic is the sum of squared differences between observed and expected data after some normalization:

$$
\Phi^2=\sum_{i=1}^I\sum_{j=1}^J\frac{(p_{ij}-p_{i\cdot}p_{\cdot j})^2}{p_{i\cdot}p_{\cdot j}}.
$$

Then, $\chi^2 = N\Phi^2$ is called the 'chi-squared' statistic and it follows a $\chi^2$ distribution with $(I-1)(J-1)$ degrees of freedom.

## Measures of association

In the most general case, how can we define the association between two categorical variables in the 2-by-2 case?

1. as a function of the cross-product of all cells (**odds-ratio**)
2. as a correlation coefficient (**chi-square test**)

Let us consider a two-way table (two binary variables):

![](./img/tab_twoway_60pct.png)


## The odds-ratio


The odds-ratio is defined as 
$$\alpha=\frac{p_{11}p_{22}}{p_{12}p_{21}},$$

or, equivalently, $\alpha=\frac{p_{11}/p_{12}}{p_{21}/p_{22}}$, when row margins (e.g., exposure) are fixed (e.g., $p_{11}/p_{12}$ = probability of disease when exposed).

This is a general measure of association, which is invariant after permutation of rows and columns.

## The chi-square

With binary variables, the previous formula for the 'chi-squared' statistic simplifies to
$$
\Phi^2=\sum_{i=1}^2\sum_{j=1}^2\frac{(p_{ij}-p_{i\cdot}p_{\cdot j})^2}{p_{i\cdot}p_{\cdot j}}=\frac{(p_{11}p_{22}-p_{21}p_{12})^2}{p_{1\cdot}p_{2\cdot}p_{\cdot 1}p_{\cdot 2}},
$$

which shows that it amounts to a simple correlation between two numerically-scored variables. 

Indeed, considering 0/1 scores, we have

$$
\rho=\frac{p_{11}p_{22}-p_{21}p_{12}}{\sqrt{p_{1\cdot}p_{2\cdot}p_{\cdot
1}p_{\cdot 2}}}=\frac{p_{22}-p_{2\cdot}p_{\cdot
2}}{\sqrt{p_{1\cdot}p_{2\cdot}p_{\cdot 1}p_{\cdot 2}}},
$$

hence, $r^2=\chi^2/N$.



## Illustration

**Caffeine consumption and marital status** `r citep(bib["dalgaard08"])`.
```{r}
coffee <- matrix(c(652,1537,598,242,36,46,38,21,
                   218,327,106,67), nrow = 3, byrow = TRUE)
dimnames(coffee) <- list("marital status" = c("Married", "Prev. married", "Single"), 
                         consumption = c("0", "1-150", "151-300", ">300"))
coffee
```

---

```{r}
round(prop.table(coffee, margin = 1), digits = 2)  ## by row
round(prop.table(coffee, margin = 2), digits = 2)  ## by column

```

---

**Bar chart**  
`barchart(table(...))`

```{r, echo=FALSE, fig.height=4.5, fig.align='center'}
library(latticeExtra)
barchart(prop.table(coffee, 1) * 100, stack = FALSE, xlab = "Proportion (%)",
         par.settings = custom.theme.2(), 
         auto.key = list(space = "top", column = 4, cex = 1))
```

---

**Dotplot**  
`dotplot(count ~ A, groups = B)`

```{r}
library(reshape2)
coffee.df <- melt(coffee, varnames = c("Status", "Caffeine"))
```


```{r, echo=FALSE, fig.height=4, fig.align='center'}
dotplot(Status ~ value, data = coffee.df, type = "o",
        groups = Caffeine, xlab = "Effectif",
        par.settings = custom.theme.2(),
        auto.key = list(x = .85, y = .95, cex = 1))
```


---

```{r}
chsq <- chisq.test(coffee)
chsq
chsq$residuals
```



# Logistic regression

## Generalized linear models

The theory of Generalized Linear Model encompasses a unified approach to regression models where a single response variable is assumed to follow a probability distribution fucntion from the **exponential family** `r citep(bib["nelder72"])`. This includes the following PDFs: Gaussian, Binomial, Poisson, Gamma, Inverse Gaussian, Geometric, and Negative Binomial.
The idea is to **'relaxe' some of the assumptions of the linear model** such that the relationship between the response and the predictors remains linear. You may recall that in the case of linear regression, we usually relate the predictors to the expected value of the outcome like so:

$$\mathbb{E}(y \mid x) = f(x; \beta),$$

or, using matrix notation, 

$$\mathbb{E}(y \mid x) = {\bf X}\beta.$$

## From linear to logistic regression

How can this be achieved with a logistic regression where individual responses are binary and follow a Bernoulli, or ${\cal B}(1;0.5)$, distribution? Moreover, a standard regression model could predict individual probabilities outside the $[0;1]$ interval.

Some transformations, like $p'=\arcsin p$, have been proposed to allow the use of ANOVA with binary data `r citep(bib["zar98"])`. However, it is fairly easy to apply a logistic regression, see also `r citep(bib["dixon08"])`.

---

Considering the logit transformation of the probability of the event under consideration, $\pi(x)=\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$, the logistic regression model is comparable to the linear case, i.e. it is additive in its effect terms. 
In the simplest case (one predictor + an intercept term), we have: 

$$g(x)=\ln\left(\frac{\pi(x)}{1-\pi(x)}\right)=\beta_0+\beta_1x.$$

```{r, echo=FALSE, fig.height=3, fig.align='center'}
set.seed(101)
n <- 200
x <- rnorm(n)
a <- 1
b <- -2
p <- exp(a+b*x)/(1+exp(a+b*x))
y <- ifelse(runif(n)>=p, 1, 0)
pp <- predict(glm(y ~ x, family = binomial), type = "response")
d <- data.frame(x, y, pp)
d <- d[order(d$x),]
xyplot(y ~ x,  data = d, xlab = "", ylab = "", 
       panel = function(x, y, ...) {
         panel.grid(h = -1, v = -1)
         panel.xyplot(x, y, col = "cornflowerblue", alpha = .5)
         panel.xyplot(x, d$pp, type = "l", col = "coral", lwd = 2)         
       })
```


## Illustration

**Prognostic study of risk factor associated with low birth infant weight** `r citep(bib["hosmer89"])`.

```{r}
data(birthwt, package = "MASS")
yesno <- c("No","Yes")
birthwt <- within(birthwt, {
  race <- factor(race, labels = c("White","Black","Other"))
  smoke <- factor(smoke, labels = yesno)
  ui <- factor(ui, labels = yesno)
  ht <- factor(ht, labels = yesno)
})
```

- `low`: whether child's weight (`bwt`) is > 2.5 kg or not.
- `ui`: presence of uterine irritability (mother).
- `age`: mother's age (when enrolled in the study).

```{r, echo = FALSE}
options(show.signif.stars = FALSE)
```

---

```{r, message=FALSE}
library(Hmisc)
summary(low ~ age + ui, data = birthwt)
```

---

```{r}
m <- glm(low ~ age, data = birthwt, family = binomial("logit"))
summary(m)
```

---

```{r, echo=FALSE, fig.height=5, fig.align='center'}
pp <- predict(m, type = "response")
d <- data.frame(x = birthwt$age, y = birthwt$low, pp)
d <- d[order(d$x),]
xyplot(y ~ x,  data = d, xlab = "Age (years)", ylab = "Birth weight status", 
       panel = function(x, y, ...) {
         panel.grid(h = -1, v = -1)
         panel.xyplot(x, jitter(y, amount = .02), col = "cornflowerblue", cex = .9, alpha = .5)
         panel.xyplot(x, d$pp, type = "l", col = "coral", lwd = 2)         
       })
```


## Fitted values

```{r}
set.seed(154)
idx <- sample(1:nrow(birthwt), 5)
birthwt$low[idx]
predict(m)[idx]
predict(m, type = "response")[idx]
```

## More than one predictor

```{r}
m2 <- glm(low ~ ui + age, data = birthwt, family = binomial("logit"))
summary(m2)
```

---

How to interpret the effet of `age` and `ui`?

- When age increases by one unit and `ui = 0` (`No`), the log-odds decrease by -0.05; the adjusted odds-ratio equals `r round(exp(coef(m2)["age"]), 3)`. The corresponding OR for a 5-year increase is `r round(exp(5*coef(m2)["age"]), 3)`.
- In presence of uterine irritability (`ui = 1`), the log-odds increase by 0.90, when age is held constant; the adjusted odds-ratio equals `r round(exp(coef(m2)["uiYes"]), 3)`.

```{r, message = FALSE}
exp(coef(m2)["uiYes"])
exp(confint(m2)["uiYes",])
```

---

Yet another way to compute the odds-ratio:

```{r, message = FALSE}
m3 <- glm(low ~ ui, data = birthwt, family = binomial("logit"))
exp(coef(m3)["uiYes"])
library(vcd)
oddsratio(xtabs(~ low + ui, data = birthwt), log = FALSE)
```


# References

## References {.smaller}

```{r, echo=FALSE, results='asis'}
bibliography()
```

