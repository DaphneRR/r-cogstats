Statistical models in R
========================================================
author: Christophe Lalanne
date: October 22, 2013
css: custom.css

```{r, include=FALSE}
library(xtable)
library(knitcitations)
cite_options(tooltip=TRUE)
bib <- read.bibtex("../refs.bib")
library(Cairo)
CairoFonts(
  regular = "Fontin Sans:style=Regular",
  bold = "Fontin Sans:style=Bold",
  italic = "Fontin Sans:style=Italic",
  bolditalic = "Fontin Sans:style=Bold Italic,BoldItalic"
)
opts_chunk$set(cache=TRUE, dev="CairoPNG")
options(reindent.spaces=2, show.signif.stars=FALSE)
library(latticeExtra)
## https://github.com/jennybc/STAT545A
my.col <- c('cornflowerblue', 'chartreuse3', 'darkgoldenrod1', 'peachpuff3',
            'mediumorchid2', 'turquoise3', 'wheat4', 'slategray2')
trellis.par.set(strip.background = list(col = "transparent"), 
                plot.symbol = list(pch = 19, cex = 1.2, col = my.col),
                plot.line = list(lwd = 2, col = my.col[1]),
                superpose.symbol = list(pch = 19, cex = 1.2, col = my.col),
                superpose.line = list(lwd = 2, col = my.col),
                box.rectangle = list(col = my.col),
                box.umbrella = list(col = my.col),
                #box.dot = list(col = my.col),
                fontsize = list(text = 16, points = 8))
set.seed(101)
```



Synopsis
========================================================
type: sub-section

The analysis of variance is not a mathematical theorem, but rather a convenient method of arranging the arithmetic. Ronald Fisher (1890-1962)

> design of experiments • split-apply-combine • one-way and two-way ANOVA • interaction

**Lectures:** OpenIntro Statistics, 4.2, 5.2, 5.5.


Design of experiments
========================================================

**Maximize precision while minimizing number of trials.**

Implementation of an organized set of experimental units to characterize the effect of certain treatments or combination of treatments, on one or more response variables. In factorial designs, for example, all levels of all experimental factors will be crossed.

Taking into account one or more nuisance factors for the establishment of experimental design: organize sources of unwanted variation so that we can say that they affect treatment equivalently, making the comparison between treatments possible.


Some examples
========================================================

* Parallel (independent) groups
* Factorial experiment
* Complete or incomplete block designs, Latin square design
* Split-plot design
* Repeated measures, including cross-over trials

Describing variables relationships
========================================================
R relies on a 'formula' to describe relation between one or multiple response variables and one or more explanatory variables, according to Wilkinson & Rogers's notation `r citep(bib[c("wilkinson73","chambers92")])`. 

In the case of ANOVA and regression, the response variable is put on the left of the `~` operator, followed by 

| RHS     | Variable type    | Meaning                     | Equiv. to             |
| ------- |:----------------:|:---------------------------:|:---------------------:|
| x       | numeric          | simple linear regression    | 1 + x                 |
| x + 0   | numeric          | idem without intercept      | x - 1                 |
| a + b   | factor (numeric) | two main crossed effects    |                       |
| a * b   | factor           | idem including interaction  | 1 + a + b + a:b       |
| a / b   | factor           | nested relationship         | 1 + a + b + a %in% b  |


R's formula and data analysis
========================================================

Most of the time, the same formula can be used to perform several 'data steps' ([tidying](http://vita.had.co.nz/papers/tidy-data.pdf) and [summarizing](http://biostat.mc.vanderbilt.edu/wiki/Main/Hmisc) data, [plotting](http://dsarkar.fhcrc.org/lattice-lab/latticeLab.pdf), or [reporting](http://yihui.name/knitr/)), but it is also the core element of many statistical models in R (linear and generalized linear models, decision trees, partitioning methods, etc.).  
The use of formulae means we also need to work with a well-arranged data frame, for which **reshaping** (melting/casting) are essential tools.

```{r}
d <- data.frame(x1 = rnorm(n=5, mean=10, sd=1.5), 
                x2 = rnorm(n=5, mean=12, sd=1.5))
d[1:3,]  # same as head(d, n=3)
```

R's formula and data analysis (con't)
========================================================

```{r}
res <- t.test(d$x1, d$x2, var.equal=TRUE)
res$p.value
library(reshape2)
dm <- melt(d)     # switch from wide to long format
head(dm)
t.test(value ~ variable, data=dm, var.equal=TRUE)$p.value
```

<small>**Note:** R's formulae (and data frames, *de facto*) have been ported to [Python](http://patsy.readthedocs.org/en/latest/) and [Julia](http://juliastats.github.io/DataFrames.jl/formulas.html).</small>


The Split-Apply-Combine strategy
========================================================

"(...) break up a big problem into manageable pieces, operate on each piece independently and then put all the pieces back together." `r citep(bib["wickham11"])`

---

![sac](./img/split_apply_combine.png)

Split-Apply-Combine (con't)
========================================================

Here is a working example:
```{r}
x <- rnorm(n=15)
grp <- gl(n=3, k=5, labels=letters[1:3])
spl <- split(x, grp)      # split x by levels of grp
apl <- lapply(spl, mean)  # apply mean() to each split 
cbn <- rbind(x=apl)       # combine the means
cbn
```

Shorter version (other than `by()`, `tapply()`, or `ave()`):
```{r}
aggregate(x ~ grp, FUN=mean)
```

One-way ANOVA
========================================================

Let $y_{ij}$ be the $j\text{th}$ observation in group $i$ (factor
$A$, with $a$ levels). An **effect model** can be written as

$$ y_{ij} = \mu + \alpha_i + \varepsilon_{ij}, $$

where $\mu$ stands for the overall (grand) mean, $\alpha_i$ is the effect of group $i$ ($i=1,\dots,a$), and $\varepsilon_{ij}\sim \mathcal{N}(0,\sigma^2)$ reflects random error. The following restriction is usually considered: 
$\sum_{i=1}^a\alpha_i=0$. 

The **null hypothesis** reads: $H_0:\alpha_1=\alpha_2=\dots=\alpha_a$. It can be tested with an F-test with $a-1$ et $N-a$ degrees of freedom. 

Assumptions, caveats, etc.
========================================================

* This is an *omnibus test* for which the alternative hypothesis reads $\exists i,j\mid \alpha_i\neq\alpha_j,\: i, j=1,\dots,a\, (i\neq j)$. If the result is significant, it doesn't tell us which pairs of means really differ.
* Beside independence of observations, this model assumes that variances are equal in each population and that residuals (response variable in this case) are approximately normally distributed.
* As always, a statistically significant result does not necessarily mean an interesting result from a practical point of view: We need a measure of effect size.


Illustration
========================================================

![anova](./img/fig-anova.png)


Application
========================================================

Effect of different sugars on length of pea sections grown in tissue culture with auxin present. `r citep(bib["sokal95"])`

```{r, echo=FALSE, results='asis'}
peas <- read.table("../data/peas.dat", header=TRUE)
print(xtable(peas), type="html")
```

Application (con't)
========================================================

* **Response variable**: length (in ocular units) of pea sections.
* **Explanatory variable**: treatment (control, 2% glucose, 2% fructose, 1% glucose + 1% fructose, 2% sucrose).

```{r}
head(peas, n=3)
peas.melted <- melt(peas, value.name="length", 
                    variable.name="treatment")
head(peas.melted, n=3)
```

Application (con't)
========================================================

```{r}
f <- function(x) c(mean=mean(x), sd=sd(x))
aggregate(length ~ treatment, data=peas.melted, f)
```
```{r, echo=FALSE, fig.width=10, fig.height=5, fig.align="center"}
bwplot(length ~ treatment, data=peas.melted,
       panel=function(x, y, ...) {
         panel.grid(v = -1, h = 0)
         panel.bwplot(x, y, col=my.col[x], pch="|", lwd=1.5, ...)
         panel.xyplot(jitter(as.numeric(x), amount=.05), y, alpha=.5, col=my.col[x], ...)
       })
```


Application (con't)
========================================================

```{r}
mod <- aov(length ~ treatment, data=peas.melted)
summary(mod)
```

Two-way ANOVA
========================================================

Let $y_{ijk}$ be the $k\text{th}$ observation for level $i$ of factor $A$
($i=1,\dots,a$) and level $j$ of factor $B$ ($j=1,\dots,b$). The full model can be written as follows:

$$ y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk}, $$

where $\mu$ is the overall mean, $\alpha_i$ ($\beta_j$) is the deviation of the $a$ ($b$) means from the overall mean,
$\gamma_{ij}$ is the deviation of the $A\times B$ treatments from $\mu$, and $\varepsilon_{ijk}\sim
{\cal N}(0,\sigma^2)$ is the residual term.

The $\alpha_i$ et $\beta_j$ are called **main effects**,
and $\gamma_{ij}$ is the **interaction effect**. 

Interaction between factors
========================================================

![anova](./img/fig-interaction.png)

References
========================================================

```{r, echo=FALSE, results='asis'}
bibliography(style="text")
```
