---
title: "Regression modeling ➊"
author: "Christophe Lalanne"
date: "November 11, 2014"
output:
  ioslides_presentation:
    css: style.css
---
  
```{r, include=FALSE}
library(knitr)
library(ascii)
library(knitcitations)
#cite_options(tooltip=TRUE)
bib <- read.bibtex("../refs.bib")
library(Cairo)
CairoFonts(
  regular = "Fontin Sans:style=Regular",
  bold = "Fontin Sans:style=Bold",
  italic = "Fontin Sans:style=Italic",
  bolditalic = "Fontin Sans:style=Bold Italic,BoldItalic"
)
opts_chunk$set(cache=TRUE, dev="CairoPNG", dev.args = list(bg = 'transparent'))
options(reindent.spaces=2)
library(latticeExtra)
## https://github.com/jennybc/STAT545A
my.col <- c('cornflowerblue', 'chartreuse3', 'darkgoldenrod1', 'peachpuff3',
            'mediumorchid2', 'turquoise3', 'wheat4', 'slategray2')
trellis.par.set(strip.background = list(col = "transparent"), 
                plot.symbol = list(pch = 19, cex = 1.2, col = my.col),
                plot.line = list(lwd = 2, col = my.col[1]),
                superpose.symbol = list(pch = 19, cex = 1.2, col = my.col),
                superpose.line = list(lwd = 2, col = my.col),
                box.rectangle = list(col = "gray60"),
                box.umbrella = list(col = "gray60"),
#                box.dot = list(col = my.col),
                fontsize = list(text = 16, points = 8))
set.seed(101)
```



# Synopsis

---
  
> The greatest value of a picture is when it forces us to notice what we never expected to see. John Tukey (1915-2000)

<br /><br /><br />
  
<center>
**correlation • simple linear regression • parameter estimation • diagnostic measures**
</center>

<br /><br /><br />
**Lectures:** OpenIntro Statistics, 7.1-7.4.


# Association measures

## Association, correlation, causality

Linear regression allows to model the relationship between a continuous outcome and one or more variables of interest, also called predictors. Unlike correlation analysis, these variables play an asymmetrical role, and usually we are interested in quantifying the strength of this relationship, as well as the amount of variance in the response variable acounted for by the predictors.

In linear regression, we assume a causal effect of the predictor(s) on the outcome. When quantifying the degree of association between two variables, however, both variables play a symmetrical role (there's no outcome or response variable). Moreover, correlation [usually](http://stats.stackexchange.com/q/534/930) does not imply causation.

## Linear correlation

([Bravais-](http://www.amstat.org/publications/jse/v9n3/stanton.html))Pearson correlation coefficient provides a unit-less measure of linear co-variation between two numeric variables, contrary to covariance which depends linearly on the measurement scale of each variable.  

A perfect positive (negative) linear correlation would yield a value of $r=+1$ ($-1$).

$$   
r=\frac{\overbrace{\sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y})}^{\text{covariance
        (x,y)}}}{\underbrace{\sqrt{\sum_{i=1}^n(x_i-\overline{x})^2}}_{\text{std deviation of x}}\underbrace{\sqrt{\sum_{i=1}^n(y_i-\overline{y})^2}}_{\text{std deviation of y}}}
$$

# Linear regression

## It's always about explained variance

We want to account for individual variations of a response variable, $Y$, considering one or more predictors or explanatory variables, $X_j$ (numeric or categorical).  **Simple linear regression** considers only one continuous predictor.

![linmod](./img/fig-linmod_25pct.png)

## A general modeling framework

Usually, we will consider that there is a **systematic and a random (residual) part** at the level of these individual variations. The linear model allows to formalize the asymmetric relationship between response variable ($Y$) and predictors ($X_j$) while separating these two sources of variation:

$$
\text{response} = \text{predictor(s) effect} + \hskip-11ex
\underbrace{\;\;\text{noise,}}_{\text{measurement error, observation period, etc.}}
$$

Importantly, this theoretical relationship is **linear and additive**: `r citep(bib[c("draper98","fox10")])`

$$ \mathbb{E}(y \mid x) = f(x; \beta) $$

## All model are wrong, some are useful

... or rather, "the practical question is how wrong do they have to be to not be useful" ([Georges Box](http://goo.gl/RN2t9C)).  

"Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise" `r citep(bib["tukey62"])`.

But all statistical models rely on more or less stringent assumptions. In the case of linear regression, the **linearity of the relationship** and **influence of individual data points** must be carefully checked.
Alternatives do exist, though: resistant or robust (Huber, LAD, quantile) methods, [MARS](http://goo.gl/Ox2xsO), [GAM](http://goo.gl/C8MR6B), or restricted cublic splines `r citep(bib[c("marrie09","harrell01")])`.

## ANOVA vs. regression

- Not so much different, in a certain sense.
- Under the GLM umbrella, we can use regression to fit an ANOVA model.

![linmod2](./img/fig-linmod2_35pct.png)

## A working example

```{r}
# Simulate 10 obs. assuming the true model is Y=5.1+1.8X. 
set.seed(101)
n <- 10
x <- runif(n, 0, 10)
y <- 5.1 + 1.8 * x + rnorm(n)  # add white noise
stem(y, scale = 2)
```

---

```{r}
summary(m <- lm(y ~ x))
```

---

```{r, echo=FALSE, fig.width=5, fig.height=5}
xyplot(y ~ x, type=c("p","g"), cex=1.2,
       col="cornflowerblue",
       panel=function(x, y, ...) {
         panel.xyplot(x, y, pch=1, ...)
         panel.lmline(x, y, col="grey80")
         panel.xyplot(x, fitted(m), pch=19, ...)
         panel.segments(x, y, x, fitted(m), col="grey80")
         panel.xyplot(mean(x), mean(y), pch=3, cex=1.5)
       })
```

## Recap'

- OLS minimizes vertical distances between observed and fitted $y$ values (residual sum of squares).
- The regression line pass through the mean point, $(\bar x, \bar y)$; $b_0=\bar y-b_1\bar x$.
- The slope of the regression line is found to be $\sum_i(y_i-\bar y)(x_i-\bar x)\big/\sum_i(x_i-\bar x)^2$. (Compare to the formula for the correlation coefficient.)

## Testing significance of slope or model

```{r}
summary(m)$coefficients
2*pt(1.619/0.136, 10-2, lower.tail=FALSE) ## t-test for slope
```


---

We might also be interested in testing the model as a whole, especially when there are more than one predictor.

```{r}
anova(m)
```

## Fitted values and residuals

Recall that **data = model fit + residuals**, as described in the ANOVA case.


```{r}
coef(m)[1] + coef(m)[2] * x[1:5]  ## adjusted values
fitted(m)[1:5]
```

---

**residuals = data - model fit**, what is not explained by the linear regression.

```{r}
y[1:5] - predict(m)[1:5]          ## residuals
resid(m)[1:5]
```


## Beyond the regression line

Predicted/adjusted values are estimated conditional on regressor values, assumed to be fixed and measured without error. 

<div class="columns-2">
We need further distributional assumption to draw inference or estimate 95% CIs for the parameters, namely that residuals follows an $\mathcal{N}(0;\sigma^2)$.  

**Residual analysis shows what has not yet been accounted for in a model**. 

![](./img/fig-linreg2_20pct.png)

</div>

## Influence measures

```{r, echo=2, fig.width=8, fig.height=4, fig.align="center"}
xyplot(resid(m) ~ fitted(m), type=c("p", "g", "smooth"), span=1, 
       col.line="orange", lwd=2, col="cornflowerblue",
       abline=list(h=0, lty=2))
```

---

```{r}
head(influence.measures(m)$infmat)  # Chambers & Hastie, 1992
```


# References

## References {.smaller}

```{r, echo=FALSE, results='asis'}
bibliography()
```



